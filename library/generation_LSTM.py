import torch
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

from library.constants import DEVICE, N_ASSETS, WINDOW_SIZE


@torch.no_grad()
def generate_samples(generator, assets: list[str], n_samples: int = 1) -> pd.DataFrame | list[pd.DataFrame]:
    """
    Generate random samples from generator
    If n_samples > 1, then generate samples with shifted noise
    """
    generator.eval()
    z = generator.get_shifted_noise(n_samples).to(DEVICE)
    samples = generator(z).cpu()
    # plt.plot(np.array(samples[15]).transpose().cumsum(axis=0))
    if n_samples == 1:
        # Return one sample
        samples = samples.squeeze()#.reshape(N_ASSETS, WINDOW_SIZE)
        assert samples.size() == (N_ASSETS, WINDOW_SIZE)
        return pd.DataFrame(samples.T, columns=assets)
    else:
        # # Return multiple sample
        # dfs = []
        # # samples = samples.reshape(n_samples, N_ASSETS, WINDOW_SIZE)
        # for sample in samples:
        #     assert sample.size() == (N_ASSETS, WINDOW_SIZE)
        #     dfs.append(pd.DataFrame(sample.T, columns=assets))
        return samples


def generate_fake_returns(generator, df_returns_real: pd.DataFrame, seed: int) -> pd.DataFrame:
    torch.random.manual_seed(seed)
    # Create list of consequent samples
    dfs = generate_samples(generator, df_returns_real.columns, len(df_returns_real) - WINDOW_SIZE + 1)
    # Merge generated DataFrames
    df_returns_fake = _merge_generated_dfs(dfs, df_returns_real)
    print('shape after merg', df_returns_fake.shape)
    # Normalize fake returns
    df_returns_fake = _normalize_returns(df_returns_fake, df_returns_real)
    return df_returns_fake


def _merge_generated_dfs(dfs: list[pd.DataFrame], df_returns_real: pd.DataFrame) -> pd.DataFrame:
    """
    Merge consequent dfs generated by TCN network
    """
    # dfs[100].cumsum().plot()

    print(f"dfs.shape = {dfs.shape}")
    result = np.concatenate(np.array(dfs), axis=1)
    print(f"result.shape = {result.shape}")
    dfs = pd.DataFrame(result[:, :df_returns_real.shape[0]].transpose(), columns=df_returns_real.columns)
    # result = [dfs[0]]
    # for i, df in enumerate(dfs[1:]):
    #     prev_df = dfs[i]
    #     # assert np.allclose(prev_df.iloc[31:].values, df.iloc[30:-1].values), f'{(~np.isclose(prev_df.iloc[30:].values, df.iloc[29:-1].values)).sum(), }'
    #     result.append(df.iloc[-1:])

    return dfs.set_index(df_returns_real.index)#pd.concat(result).set_index(df_returns_real.index)


def _normalize_returns(df_returns_fake: pd.DataFrame, df_returns_real: pd.DataFrame) -> pd.DataFrame:
    """
    Normalize generated samples to have the same mean and standard deviation as real returns
    """
    print(df_returns_fake)
    # Normalize to N(0, 1)
    df_returns_fake = df_returns_fake.sub(df_returns_fake.mean(), axis=1)
    df_returns_fake = df_returns_fake.div(df_returns_fake.std(), axis=1)
    # Normalize to N(mu, sigma)
    df_returns_fake = df_returns_fake.mul(df_returns_real.std(), axis=1)
    df_returns_fake = df_returns_fake.add(df_returns_real.mean(), axis=1)
    # Sanity check
    assert np.allclose(df_returns_fake.mean(), df_returns_real.mean()) and np.allclose(df_returns_fake.std(), df_returns_real.std())
    return df_returns_fake
